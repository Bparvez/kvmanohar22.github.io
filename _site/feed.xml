<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-06-11T22:51:53+05:30</updated><id>http://localhost:4000/</id><title type="html">KV Manohar</title><entry><title type="html">Weight Initialization</title><link href="http://localhost:4000/Weight_Init/" rel="alternate" type="text/html" title="Weight Initialization" /><published>2017-04-21T00:00:00+05:30</published><updated>2017-04-21T00:00:00+05:30</updated><id>http://localhost:4000/Weight_Init</id><content type="html" xml:base="http://localhost:4000/Weight_Init/">&lt;p&gt;Initializing weights randomly is one of &lt;b&gt;the&lt;/b&gt; most important tasks in training a deep neural network.
You can find &lt;a href=&quot;https://github.com/kvmanohar22/DeepNets&quot;&gt;here&lt;/a&gt; how weights affect the internal working of a neural network.
I’ll update this post with some more interesting things related to weight initialization.
&lt;br /&gt; That’s it for now! Until next time!!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://kvmanohar22-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</content><author><name></name></author><summary type="html">Initializing weights randomly is one of the most important tasks in training a deep neural network. You can find here how weights affect the internal working of a neural network. I’ll update this post with some more interesting things related to weight initialization. That’s it for now! Until next time!!</summary></entry><entry><title type="html">GSoC</title><link href="http://localhost:4000/GSoC/" rel="alternate" type="text/html" title="GSoC" /><published>2017-04-04T00:00:00+05:30</published><updated>2017-04-04T00:00:00+05:30</updated><id>http://localhost:4000/GSoC</id><content type="html" xml:base="http://localhost:4000/GSoC/">&lt;p&gt;Hey there !! &lt;br /&gt;&lt;br /&gt;
I’m applying for Google Summer of Code 2017 under &lt;a href=&quot;https://www.opencv.org&quot;&gt;OpenCV&lt;/a&gt; where
I plan to work on the task of object detection by developing highly compact models. You can find my proposal &lt;a href=&quot;https://docs.google.com/document/d/1zUxaQ4WYM211WaS17Dbe1jHlNeN7Twt0Aws5tt1zKXU/edit?usp=sharing&quot;&gt;here&lt;/a&gt;. I will blog the progress of my GSoC project here!&lt;br /&gt;
Yayy! Got selected to Google Summer of Code 2017. Looking forward to a great 
summer !&lt;/p&gt;

&lt;h2 id=&quot;community-bonding-period&quot;&gt;Community Bonding Period&lt;/h2&gt;
&lt;p&gt;As outlined in my proposal I’ll be working on the task of Object Detection which
involves generating bounding boxes for the detected objects in an image. I’ve 
divided my entire timeline into three major phases which includes: &lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Training a Deep Learning Model on ImageNet dataset for Image Classification&lt;/li&gt;
  &lt;li&gt;Adding additional layers to the above trained model and retraining on PASCAL
dataset&lt;/li&gt;
  &lt;li&gt;Implement Deep Compression methods to reduce the size of the model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;During this period I spent my time in setting up my developement environment on the server by installing &lt;a href=&quot;https://github.com/BVLC/caffe&quot;&gt;Caffe&lt;/a&gt; Deep Learning framework, downloading the humongous &lt;a href=&quot;http://image-net.org/&quot;&gt;ILSVRC-2012 dataset&lt;/a&gt;, reading up the research papers outlined in my proposal. 
Caffe is a great framework but what it lacks is documentation and tutorials for a beginner :(. Constrained to implement my project in C++,  I had to go through the &lt;a href=&quot;http://caffe.berkeleyvision.org/doxygen/annotated.html&quot;&gt;official API&lt;/a&gt; and figure out myself what’s happening under the hood. 
Luckily I had a bit of experience working on Google Protocol Buffers which is largely used in Caffe for defining various core classes. 
And since I had some time to kill during this period, I thought “Hey, why not create few tutorials ?” and so began the saga of writing few tutorials aimed to kick start the understanding of Caffe framework which I’ll be releasing pretty soon once I complete them. This pretty much sums up my Community Bonding Period.&lt;/p&gt;

&lt;h2 id=&quot;phase-1-of-gsoc--training-squeezenet-model-on-imagenet&quot;&gt;Phase 1 of GSoC : Training SqueezeNet Model on ImageNet&lt;/h2&gt;
&lt;p&gt;In general Deep Learning models take up huge chunk of memory to hold the learnable parameters. But SqueezeNet is designed so as to reduce the number of parameters in the following ways:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Clever use of &lt;strong&gt;1x1&lt;/strong&gt; kernels instead of &lt;strong&gt;3x3&lt;/strong&gt; kernels which reduce the number of parameters of that kernel by a factor of 9 !&lt;/li&gt;
  &lt;li&gt;Usage of global pooling instead of fully connected layers for the final classification task&lt;/li&gt;
  &lt;li&gt;Decreasing the number of input channels to &lt;strong&gt;3x3&lt;/strong&gt; kernels by introducing squeeze module&lt;/li&gt;
  &lt;li&gt;Downsampling the spatial resolution late in the network so that convolutional layers have large activation maps. This intuitively ensures that the kernels early and deeper in the network can learn more features&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This leads to a network with approx. &lt;strong&gt;1.2 million&lt;/strong&gt; (&lt;strong&gt;4.78MB&lt;/strong&gt; size uncompressed network) parameters which is considerably less compared to other popular networks such as AlexNet, GoogleNet, ZFNet, VGGNet, ResNet all of which have orders of \(10s\) of millions of parameters.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./../images/Net_Det.png&quot; alt=&quot;Architecture&quot; /&gt;
&lt;strong&gt;Fig 1&lt;/strong&gt;: &lt;strong&gt;Left&lt;/strong&gt; Architecture of SqueezeNet. &lt;strong&gt;Right&lt;/strong&gt; Architecture of SqueezeDet.&lt;/p&gt;

&lt;p&gt;Now let’s come to the part of training SqueezeNet for Image Classification task. ImageNet is &lt;strong&gt;the&lt;/strong&gt; biggest dataset of images out there of size nearly &lt;strong&gt;1.2 TB&lt;/strong&gt; ! ILSVRC (ImageNet Large Scale Visual Recognition Competition) is a collection of subset of images from ImageNet with nearly 1.2 million images (approx. 1k images per class). There were number of challenges that I faced while fine tuning the right set of hyperparameters to kick start the training. One key problem was with &lt;strong&gt;Dead ReLUs&lt;/strong&gt; deeper in the network which continously output \(0\). This leads to the network to a never changing state. Catching this problem was pretty easy by analyzing the training loss as a function of number of iterations and analyzing the distributions of activation maps of Convolutional layers. Here are some of the visualizations during the training phase.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./../images/failed_training.jpg&quot; alt=&quot;Failed Training&quot; /&gt; 
&lt;strong&gt;Fig 2&lt;/strong&gt;: Training which didn’t converge. &lt;strong&gt;Top row:&lt;/strong&gt; Histogram distribution of activation maps of conv layers at the top of the network. &lt;strong&gt;Middle row:&lt;/strong&gt; Loss function which didn’t converge. &lt;strong&gt;Bottom row:&lt;/strong&gt; Histogram distribution of activation maps of conv layers towards the end of the network. The difference between distributions in the top and bottom row clearly indicate the problem with &lt;strong&gt;Dead ReLUs&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./../images/successful_training.jpg&quot; alt=&quot;Successful Training&quot; /&gt;
&lt;strong&gt;Fig 3&lt;/strong&gt;: Training which did converge !&lt;/p&gt;

&lt;p&gt;So, why the problem with &lt;strong&gt;Dead ReLUs&lt;/strong&gt; and how to get over it? There could be many reasons such as bad weight initialization (You might want to have a look at &lt;a href=&quot;https://github.com/kvmanohar22/DeepNets&quot;&gt;this&lt;/a&gt;), high learning rate and so on. The solution to this could be to use Leaky ReLU (should be experimented, worked in my case), try with smaller learning rate. Let’s dive into some math here.
Mathematically,&lt;/p&gt;

&lt;p&gt;Let the activation map of Conv layer be denoted by \(A_i\) for the \(i\) th kernel. Here \(x_{jk}\) is the input tensor to the conv layer.&lt;/p&gt;

&lt;p&gt;\(A_i = \sum W_i*x_{jk}\)&lt;/p&gt;

&lt;p&gt;Applying ReLU non-linearity element-wise results in \(B_i\) where,&lt;/p&gt;

&lt;p&gt;\(B_i = max(0, A_i) \)&lt;/p&gt;

&lt;p&gt;During the backward pass, we have \(\Large \frac{\partial{L}}{\partial{B_i}}\) coming from the bottom layer (\(L\) being the loss function). To update the weights \(W_i\) we need to compute \(\Large \frac{\partial{L}}{\partial{W_i}}\) which is done as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial{L}}{\partial{A_i}}=\frac{\partial{L}}{\partial{B_i}} \frac{\partial{B_i}}{\partial{A_i}} \quad and \quad \frac{\partial{B_i}}{\partial{A_i}} = \begin{cases}1 &amp; \text{if $A_i$ $\gt$ $0$} \\[2ex]
0 &amp; \text{otherwise}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial{L}}{\partial{W_i}} = \frac{\partial{L}}{\partial{A_i}}\frac{\partial{A_i}}{\partial{W_i}} \quad and \quad \frac{\partial{A_i}}{\partial{W_i}} = x_{jk}&lt;/script&gt;

&lt;p&gt;Finally the weight update rule (without momentum and \(\alpha\) being learning rate) gives us:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
W_i = 
\begin{cases}
W_i - \Large \alpha x_{jk} \frac{\partial{L}}{\partial{B_{i}}} &amp; \text{if $A_{i}$ $\gt$ $0$} \\[2ex]
W_i &amp; \text{otherwise}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;This above weight update rule clearly indicates why the weights of &lt;strong&gt;Dead ReLUs&lt;/strong&gt; never change and thus the constant loss \(vs\) iterations in &lt;strong&gt;Fig 2&lt;/strong&gt; above.&lt;/p&gt;

&lt;h2 id=&quot;phase-2-of-gsoc--training-squeezedet-model-on-pascal&quot;&gt;Phase 2 of GSoC : Training SqueezeDet Model on PASCAL&lt;/h2&gt;

&lt;p&gt;Loss function is as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;loss = \frac{\lambda_{bbox}}{N_{obj}}\sum_{i=1}^{W}\sum_{j=1}^{H}\sum_{k=1}^{K}I_{ijk}[(\delta x_{ijk}-\delta x_{ijk}^{G})^2+(\delta y_{ijk}-\delta y_{ijk}^{G})^2 \\ 
+(\delta w_{ijk}-\delta w_{ijk}^{G})^2+(\delta h_{ijk}-\delta h_{ijk}^{G})^2] \\ 
+\sum_{i=1}^{W}\sum_{j=1}^{H}\sum_{k=1}^{K}\frac{\lambda_{conf}^{+}}{N_{obj}}I_{ijk}(\gamma_{ijk}-\gamma_{ijk}^{G})^2+\frac{\lambda_{conf}^{-}}{WHK-N_{obj}}\overline{I}_{ijk}\gamma_{ijk}^{2}\\
+\frac{1}{N_{obj}}\sum_{i=1}^{W}\sum_{j=1}^{H}\sum_{k=1}^{K}\sum_{c=1}^{C}I_{ijk}l_{c}^{G}\log(p_{c})&lt;/script&gt;

&lt;p&gt;This can be split into three major parts where the first part contains &lt;strong&gt;bounding box regression&lt;/strong&gt;, the second part contains &lt;strong&gt;confidence regression&lt;/strong&gt; and the last part is just the &lt;strong&gt;cross entropy loss&lt;/strong&gt; for classification.&lt;/p&gt;

&lt;h2 id=&quot;phase-3-of-gsoc--compress-and-re-train-the-initial-network&quot;&gt;Phase 3 of GSoC : Compress and re-train the initial Network&lt;/h2&gt;
&lt;p&gt;This involves application of three major methods:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Network Pruning&lt;/li&gt;
  &lt;li&gt;Quantization&lt;/li&gt;
  &lt;li&gt;Huffman Coding&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- &lt;br&gt;&lt;br&gt;
&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;
(function() {
var d = document, s = d.createElement('script');
s.src = 'https://kvmanohar22-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;
&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;
--&gt;
&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;</content><author><name></name></author><summary type="html">Hey there !! I’m applying for Google Summer of Code 2017 under OpenCV where I plan to work on the task of object detection by developing highly compact models. You can find my proposal here. I will blog the progress of my GSoC project here! Yayy! Got selected to Google Summer of Code 2017. Looking forward to a great summer !</summary></entry><entry><title type="html">YOLO</title><link href="http://localhost:4000/YOLO/" rel="alternate" type="text/html" title="YOLO" /><published>2017-03-18T00:00:00+05:30</published><updated>2017-03-18T00:00:00+05:30</updated><id>http://localhost:4000/YOLO</id><content type="html" xml:base="http://localhost:4000/YOLO/">&lt;p&gt;&lt;b&gt;YOLO&lt;/b&gt; (You Only Look Once) is an Unified Real-Time Object Detection algorithm. You can find the original paper on &lt;a href=&quot;https://arxiv.org/abs/1506.02640&quot;&gt;arxiv&lt;/a&gt;. I plan on implementing this paper in python using &lt;a href=&quot;https://github.com/tensorflow/tensorflow&quot;&gt;tensorflow&lt;/a&gt; deep learning framework. You can find the progress of this project code &lt;a href=&quot;https://www.github.com/kvmanohar22/YOLO-tf&quot;&gt;here&lt;/a&gt;. I’ll update this blog as and when I make substantial progress.
&lt;br /&gt;&lt;br /&gt;
That’s it for now! Until next time!!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://kvmanohar22-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</content><author><name></name></author><summary type="html">YOLO (You Only Look Once) is an Unified Real-Time Object Detection algorithm. You can find the original paper on arxiv. I plan on implementing this paper in python using tensorflow deep learning framework. You can find the progress of this project code here. I’ll update this blog as and when I make substantial progress. That’s it for now! Until next time!!</summary></entry></feed>